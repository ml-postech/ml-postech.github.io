<script> 
    /* Function to use lastModified property */ 
    document.getElementById("last-updated").innerHTML = formatDate(document.lastModified);

    function formatDate(date) {
        var d = new Date(date),
        month = '' + (d.getMonth() + 1),
        day = '' + d.getDate(),
        year = d.getFullYear();

        if (month.length < 2) 
            month = '0' + month;
        if (day.length < 2) 
            day = '0' + day;

        return [year, month, day].join('-');
    }


    function randomizeItems(items)
    {
        var cached = items.slice(0), temp, i = cached.length, rand;
        while(--i)
        {
            rand = Math.floor(i * Math.random());
            temp = cached[rand];
            cached[rand] = cached[i];
            cached[i] = temp;
        }
        return cached;
    }

    function randomizeList()
    {
        var list = document.getElementById("rpList");
        var nodes = list.children, i = 0;
        nodes = Array.prototype.slice.call(nodes);
        nodes = randomizeItems(nodes);
        while(i < nodes.length)
        {
            list.appendChild(nodes[i]);
            ++i;
        }
        list.style.display="block";
    }

    randomizeList()
</script> 


<div class="content container">
  <div class="row" style="padding-top: 40px;">
    <div class="col-sm-12">
        <h3 class="header"># Research Participation (for Undergrad Students) </h3>
        <p class="sub-text">
            MLLab always welcomes undergraduate students to participate in research. The projects below are the research topics that the lab is currently interested in and intends to proceed. Interested undergraduate students should check the list before applying for research participation and contact their advisor via email. This page will be continuously updated. (Last modified <span id="last-updated"></span>)
        </p>
    </div>
</div>


<div class="row" style="padding-top: 40px;">
    <div class="col-sm-2" >
        <img class="img-responsive img-header" src="img/reseach_participate/smiley.png" alt=""  />
    </div>
    <div class="col-sm-10">
        <h4 class="header"># Machine Learning Free Topics - Advisors: All </h4>
        <p class="normal-text">
            Any other interesting research ideas other than the topics listed below? It looks interesting, but are you wondering if this is also machine learning research? If you want to learn machine learning but don't know where to start, feel free to contact us anytime!
        </p>
    </div>
</div>

<ul id="rpList" style="list-style-type:none; padding-left: 0;">
    <li>
      <div class="row" style="padding-top: 40px;">
        <div class="col-sm-2" >
            <img class="img-responsive img-header" src="img/reseach_participate/unlearning.png" alt=""  />
        </div>
        <div class="col-sm-10">
            <h4 class="header"># Machine Unlearning - Advisors: Jungseul Ok / Sangdon Park </h4>
            <p class="normal-text">
                As shown in the recent case of 이루다, machine learning algorithms always have problems that can expose personal and sensitive information. The simplest way to solve this problem is to train the model anew after removing such sensitive information, but this re-learning is only a workaround for solving problems that may arise at any time. Machine Unlearning is a methodology for selectively removing specific information from an already trained model. Students will study what conditions must be satisfied for successful unlearning through participation in this study.
            </p>
            <span class="sub-text"> References </span>
            <ol class="sub-text">
                <li> Nguyen, Quoc Phong, Bryan Kian Hsiang Low, and Patrick Jaillet. <a href="https://arxiv.org/abs/2010.12883">"Variational Bayesian Unlearning."</a> NuerIPS 2020. </li>
                <li> Golatkar, Aditya, Alessandro Achille, and Stefano Soatto. <a href="https://arxiv.org/abs/1911.04933">"Eternal Sunshine of the Spotless Net: Selective Forgetting in Deep Networks."</a> CVPR 2020.</li>
                <li> Yoon, Youngsik, et al. <a href="https://arxiv.org/abs/2205.15567">"Few-Shot Unlearning by Model Inversion."</a> arXiv preprint 2022</li>
            </ol>
        </div>
    </div>
</li>

  <li>
    <div class="row" style="padding-top: 40px;">
      <div class="col-sm-2" >
          <img class="img-responsive img-header" src="img/reseach_participate/unlearning.png" alt=""  />
      </div>
      <div class="col-sm-10">
          <h4 class="header"># Machine Unlearning in Text-to-Image Generative Models - Advisor: Dongwoo Kim, Mentor: Saemi Moon, Minjong Lee </h4>
          <p class="normal-text">
            Generative models have made significant advancements in producing high-quality images, but they also pose risks by potentially generating harmful, explicit, or privacy- and copyright-infringing content. To address these concerns, we aim to develop methods that allow models to “unlearn” problematic concepts, thereby enhancing the ethical and safe deployment of these models.
          </p>
          <span class="sub-text"> References </span>
          <ol class="sub-text">
              <li> Moon, Saemi, Seunghyuk Cho, and Dongwoo Kim. <a href="https://arxiv.org/abs/2205.15567">"Feature Unlearning for Pre-trained GANs and VAEs."</a> AAAI 2024</li>
              <li> Kumari, Nupur, et al. <a href="https://arxiv.org/abs/2303.13516">"Ablating concepts in text-to-image diffusion models."</a> NuerIPS 2023. </li>
              <li> Gandikota, Rohit, et al. <a href="https://arxiv.org/abs/2303.07345">"Erasing Concepts from Diffusion Models."</a> ICCV 2023.</li>
          </ol>
      </div>
  </div>
</li>

<li>
  <div class="row" style="padding-top: 40px;">
    <div class="col-sm-2">
        <img class="img-responsive img-header" src="img/reseach_participate/federated_learning.png" alt=""  />
    </div>
    <div class="col-sm-10">
        <h4 class="header"># Privacy Leakage in Federated Learning - Advisor: Jungseul Ok </h4>
        <p class="normal-text">
            Federated Learning is a machine learning framework that enables multiple users to learn large-scale models without worrying about personal information leakage by enabling learning without sending personal data to a central server. For this large-scale learning, only the learning signal (the differential value of the neural net model) is transmitted to the server, not the data transmission, and according to various recent research results, even this method is not 100% safe from personal information leakage. Participation in this study aims to find out under what circumstances information leakage can be aggravated and what methodologies can be used to solve these problems.
            <!--            연합학습은 개인데이터를 중앙 서버로 보내지 않고서도 학습을 가능하게 함으로써 여러 사용자들이 개인정보 유출 걱정없이 대규모 모델을 학습하는데 힘을 보탤수 있는 기계학습 프레임워크 입니다. 이러한 대규모 학습을 위해 데이터 전송이 아닌 학습 시그널(뉴럴넷 모델의 미분값)만을 서버로 전송하는데 최근 다양한 연구결과들에 따르면 이러한 방식조차 개인정보유출로부터 100% 안전한 상황이 아니라는 것이 밝혀졌습니다. 본 연구참여는 어떤 상황에서 정보 유출이 심화될 수 있으며 이러한 문제를 해결하기 위해 어떤 방법론들이 사용될 수 있는지에 대하여 알아보는 것을 목표로 합니다. -->
        </p>
        <span class="sub-text"> References </span>
        <ol class="sub-text">
            <li> Jinwoo Jeon*, Jaechang Kim*, Kangwook Lee, Sewoong Oh, and Jungseul Ok, <a href="https://arxiv.org/abs/2110.14962">"Gradient Inversion with Generative Image Prior"</a>, NeurIPS, 2021  </li>
        </ol>        
    </div>
</div>
</li>

<li>
  <div class="row" style="padding-top: 40px;">
    <div class="col-sm-2">
        <img class="img-responsive img-header" src="img/reseach_participate/LLM_sound.png" alt=""  />
    </div>
    <div class="col-sm-10">
        <h4 class="header"># Language Aided Audio Style Transformation - Advisor: Jungseul Ok </h4>
        <p class="normal-text">
            Multi-modal deep learning trains models to perform diverse tasks by using data from various modalities like text, images, audio, and sensors. Our research, focused on Language-Guided Audio Style Transformation, aims to modify sound clips using text queries (e.g., "change it to an electric guitar") and sound data (e.g., acoustic guitar clips). The model will be able to transform the style of arbitrary sounds by applying knowledge learned from known sources, even if target sounds are not explicitly included in the training data.
        </p>
        <span class="sub-text"> References </span>
        <ol class="sub-text">
            <li> Xubo Liu, et al. <a href="https://arxiv.org/abs/2203.15147">"Separate What You Describe: Language-Queried Audio Source Separation"</a> Interspeech 2022. </li>
            <li> Eric Grinstein, et al. <a href="https://arxiv.org/abs/1710.11385">"Audio style transfer"</a> ICASSP 2018. </li>
        </ol>        
    </div>    
</div> 
</li>

<li>
 <div class="row" style="padding-top: 40px;">
    <div class="col-sm-2">
        <img class="img-responsive img-header" src="img/reseach_participate/DRL.png" alt=""  />
    </div>
    <div class="col-sm-10">
        <h4 class="header"># Data-Efficient Deep Reinforcement Learning using Data Augmentation - Advisor: Jungseul Ok </h4>
        <p class="normal-text">
            Deep reinforcement learning (RL) has demonstrated superior performance over traditional RL methods by leveraging deep neural networks. However, training deep RL agents often necessitates a substantial number of interactions with the environment, leading to low sample-efficiency. This challenge is further exacerbated in specific scenarios:
            1) when the agent must process high-dimensional sensory inputs, such as images, within a sparse-reward environment.
            2) when multiple agents are simultaneously learning in cooperative and/or competitive multi-agent systems.
            In this work, students aim to increase sample-efficiency of deep RL using effective and dynamic data augmentation strategies.
        </p>
        <span class="sub-text"> References </span>
        <ol class="sub-text">
            <li> Nicklas Hansen, and Xiaolong Wang. <a href="https://arxiv.org/abs/2011.13389">"Generalization in Reinforcement Learning by Soft Data Augmentation"</a> ICRA 2021. </li>
            <li> Sumyeong Ahn, et al.<a href="https://arxiv.org/abs/2302.05499">"CUDA: Curriculum of Data Augmentation for Long-Tailed Recognition"</a> ICLR 2023. </li>
        </ol>
    </div>    
</div> 
</li>

<li>
    <div class="row" style="padding-top: 40px;">
       <div class="col-sm-2">
           <img class="img-responsive img-header" src="img/reseach_participate/rl_exploration.png" alt=""  />
       </div>
       <div class="col-sm-10">
           <h4 class="header"># Exploration with Prior in Reinfocement Learning - Advisor: Jungseul Ok </h4>
           <p class="normal-text">
            Exploration plays a key element in Reinforcement Learning, as it allows agents to adapt to new environments while aiming to discover optimal policies. While traditional methods like epsilon-greedy, involving random actions, work in general settings, they often be inefficient when prior knowledge is available. Therefore, in this study, we focuse on developing sample-efficient exploration strategies by leveraging prior knowledge in diverse contexts, including structured bandit problems and goal-conditioned RL.
           </p>
           <span class="sub-text"> References </span>
           <ol class="sub-text">
               <li> Peter Auer, et al. <a href="https://link.springer.com/article/10.1023/A:1013689704352">"Finite-time Analysis of the Multiarmed Bandit Problem"</a> Machine Learning 2002. </li>
               <li> Jungseul Ok, et al. <a href="https://arxiv.org/abs/1806.00775">"Exploration in Structured Reinforcement Learning"</a> NeurIPS 2018. </li>
               <li> Allison C.Tam, et al.<a href="https://arxiv.org/abs/2204.05080">"Semantic Exploration from Language Abstractions and Pretrained Representations"</a> NeurIPS 2022. </li>
           </ol>
       </div>    
   </div> 
   </li>


<li>
  <div class="row" style="padding-top: 40px;">
    <div class="col-sm-2" >
        <img class="img-responsive img-header" src="img/reseach_participate/crowdsourcing.png" alt="" />
    </div>
    <div class="col-sm-10">
        <h4 class="header"># Deep Learning with Human-data - Advisor: Jungseul Ok </h4>
        <p class="normal-text">
            Deep learning achieves outstanding performance across diverse domains by utilizing large datasets. However, labeling such large datasets is prohibitively time-consuming and labor-intensive. Furthermore, as human has non-zero probability to make mistakes, we easily find noisy labels within widely used datasets including ImageNet and PASCAL VOC. In this work, we aim to design efficient frameworks considering various factors of human-generated data.
        </p>
        <span class="sub-text"> References </span>
        <ol class="sub-text">
            <li> Hoyoung Kim*, Seunghyuk Cho*, Dongwoo Kim, and Jungseul Ok, <a href="https://arxiv.org/abs/2111.00734">"Robust Deep Learning from Crowds with Belief Propagation"</a> AISTATS 2022. </li>
            <li> Hoyoung Kim, Minhyeon Oh, Sehyun Hwang, Suha Kwak, and Jungseul Ok, <a href="https://arxiv.org/abs/2303.16817">"Adaptive Superpixel for Active Learning in Semantic Segmentation"</a> ICCV 2023. </li>
            <li> Hoyoung Kim, Sehyun Hwang, Suha Kwak, and Jungseul Ok, <a href="https://arxiv.org/abs/2403.10820">"Active Label Correction for Semantic Segmentation with Foundation Models"</a> ICML 2024. </li>            
        </ol>
    </div>
</div>
</li>


<li>
  <div class="row" style="padding-top: 40px;">
    <div class="col-sm-2" >
      <img class="img-responsive img-header" src="img/reseach_participate/gps.png" alt="" />
    </div>
    <div class="col-sm-10">
      <h4 class="header"># Geometry Problem Solving with Vision Language Model - Advisor: Dongwoo Kim, Mentor: Seunghyuk Cho </h4>
      <p class="normal-text">
        Large language model (LLM) shows impressive performance on highschool-level mathematical reasoning, i.e., calculus and algebra.
        However, LLM is still weak at certain areas of mathematical reasoning, e.g., geometry.
        In this work, we incorporate visual information of the geometry problem and vision language model (VLM) to enhance the mathematical reasoning performance on geometry problems.
      </p>
      <span class="sub-text"> References </span>
      <ol class="sub-text">
        <li>Trinh, T.H., Wu, Y., Le, Q.V. et al., <a href="https://www.nature.com/articles/s41586-023-06747-5">"Solving olympiad geometry without human demonstrations"</a> Nature 2024. </li>
        <li>Renrui Zhang, et al. <a href="https://arxiv.org/abs/2403.14624">"MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual Math Problems?"</a> ECCV 2024. </li>
        <li>Renrui Zhang, et al. <a href="https://arxiv.org/abs/2407.08739">"MAVIS: Mathematical Visual Instruction Tuning"</a> arXiv 2024. </li>
      </ol>
    </div>
  </div>
</li>

<li>
  <div class="row" style="padding-top: 40px;">
    <div class="col-sm-2" >
      <img class="img-responsive img-header" src="img/reseach_participate/gle.png" alt="" />
    </div>
    <div class="col-sm-10">
      <h4 class="header"># Graph Learning Enhancement by Structural Modifications - Advisor: Dongwoo Kim, Mentor: Jaeseung Heo </h4>
      <p class="normal-text">
        Using high-quality data is crucial for training accurate machine learning models. Graph data, which includes structural information in the form of edges, offers unique opportunities for improving model performance. Graph learning enhancement aims to refine this structural information by removing noise and emphasizing more informative patterns. Techniques such as edge rewiring, which modifies connections between nodes, node dropping, which removes less important nodes, and label smoothing, which adjusts labels to reduce overconfidence, all leverage the inherent relationships in the graph to improve model accuracy and generalization.
      </p>
      <span class="sub-text"> References </span>
      <ol class="sub-text">
	<li>Yu Rong, et al., <a href="https://arxiv.org/abs/1907.10903">"DropEdge: Towards Deep Graph Convolutional Networks on Node Classification"</a> ICLR 2020. </li>
        <li>Xiaotian Han, et al., <a href="https://proceedings.mlr.press/v162/han22c.html">"G-Mixup: Graph Data Augmentation for Graph Classification"</a> ICML 2022. </li>
        <li>Kaixiong Zhou, et al. <a href="https://arxiv.org/abs/2108.13555">"Adaptive Label Smoothing To Regularize Large-Scale Graph Training"</a> SDM 2023. </li>
      </ol>
    </div>
  </div>
</li>

<li>
  <div class="row" style="padding-top: 40px;">
    <div class="col-sm-2" >
      <img class="img-responsive img-header" src="img/reseach_participate/AIforScience.png" alt="" />
    </div>
    <div class="col-sm-10">
      <h4 class="header"># Deep Learning Application for Scientific Discovery - Advisor: Dongwoo Kim, Mentor: Seungbeom Lee </h4>
      <p class="normal-text">
        This research focuses on applying deep learning to the field of scientific discovery. Specifically, we aim to build model architectures tailored to each scientific domain while efficiently incorporating inductive biases, such as geometric priors, to address various scientific challenges. 
        Our work has broad applications in areas such as molecule generation, molecular property prediction and drug-target binding pose estimation.  
      </p>
      <span class="sub-text"> References </span>
      <ol class="sub-text">
        <li>Ying, Chengxuan, et al. <a href="https://arxiv.org/abs/2106.05234">"Do Transformers Really Perform Bad for Graph Representation?"</a> NeurIPS 2021. </li>
        <li>Corso, Gabriele, et al.  <a href="https://arxiv.org/abs/2210.01776">"DiffDock: Diffusion Steps, Twists, and Turns for Molecular Docking"</a> ICLR 2023. </li>
        <li>Xu, Minkai, et al. <a href="https://arxiv.org/abs/2203.02923">"Geodiff: A geometric diffusion model for molecular conformation generation."</a> ICLR 2022. </li>
      </ol>
    </div>
  </div>
</li>

<li>
    <div class="row" style="padding-top: 40px;">
      <div class="col-sm-2" >
        <img class="img-responsive img-header" src="img/reseach_participate/PRMs.png" alt="" />
      </div>
      <div class="col-sm-10">
        <h4 class="header"># Reward Models for Personalized LLMs - Advisor: Dongwoo Kim, Mentor: Youngbin Choi </h4>
        <p class="normal-text">
            Reinforcement Learning from Human Feedback (RLHF) allows models to generate outputs aligned with human preferences, such as helpfulness and harmfulness. However, current reward models tend to reflect majority preferences, often neglecting those of individuals or minority groups. This study aims to create personalized reward models to enable the development of more individualized language models (LLMs).
        </p>
        <span class="sub-text"> References </span>
        <ol class="sub-text">
          <li>Chakraborty, Qiu, et al., <a href="https://openreview.net/pdf?id=8tzjEMF0Vq">"MaxMin-RLHF: Towards Equitable Alignment of Large Language Models with Diverse Human Preferences"</a> ICML 2024. </li>
          <li>Siththaranjan, Laidlaw, et al. <a href="https://arxiv.org/pdf/2312.08358">"Distributional Preference Learning: Understanding and Accounting for Hidden Context in RLHF"</a> ICLR 2024. </li>
          <li>Poddar, Wan, et al. <a href="https://www.arxiv.org/pdf/2408.10075">"Personalizing Reinforcement Learning from Human Feedback with Variational Preference Learning"</a> arXiv 2024. </li>
        </ol>
      </div>
    </div>
  </li>
	<!--
<li>
  <div class="row" style="padding-top: 40px;">
    <div class="col-sm-2" >
        <img class="img-responsive img-header" src="img/reseach_participate/gnn.png" alt="" />
    </div>
    <div class="col-sm-10">
        <h4 class="header"># Graph Neural Network - Advisors: Dongwoo Kim </h4>
        <p class="normal-text">
            Graph neural networks (GNNs) generalize the existing deep learning techniques to graph structured data including biological networks, molecular graphs, academic networks, and knowledge graphs. In this project, we aim to improve the existing GNNs to be more expressive and sample-efficient. We are particularly interested in directions like architecture design, data augmentation, and self-supervised learning. 
        </p>
        <span class="sub-text"> References </span>
        <ol class="sub-text">
            <li> Kipf, Thomas N., and Max Welling. <a href="https://arxiv.org/abs/1609.02907">"Semi-supervised Classification with Graph Convolutional Networks."</a> ICLR 2017. </li>
            <li> Xu, Keyulu, et al. <a href="https://openreview.net/forum?id=ryGs6iA5Km">"How Powerful are Graph Neural Networks?."</a> ICLR 2019. </li>            
        </ol>
    </div>
</div>
</li>
	

<li>
  <div class="row" style="padding-top: 40px;">
    <div class="col-sm-2" >
        <img class="img-responsive img-header" src="img/reseach_participate/poincare_disk.png" alt="" />
    </div>    
    <div class="col-sm-10">
      <h4 class="header"># Machine Learning in Non-Euclidean Spaces - Advisor: Dongwoo Kim </h4>
      <p class="normal-text">
        Most of well-known machine learning algorithms are working in Euclidean space. However, many scientific fields study data whose underlying structure is non-Euclidean. Through this research paticipation, students will study different types of non-Euclidean spaces and develop a new machine learning algorithm working in these spaces.
    </p>
    <span class="sub-text"> References </span>
    <ol class="sub-text">
        <li> Michael M Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, Pierre Vandergheynst, <a href="https://ieeexplore.ieee.org/abstract/document/7974879"> Geometric Deep Learning: Going beyond Euclidean data</a> </li>
        <li> Seunghyuk Cho, Juyong Lee, Jaesik Park, Dongwoo Kim, <a href="https://arxiv.org/abs/2205.13371">A Rotated Hyperbolic Wrapped Normal Distribution for Hierarchical Representation Learning</a> NeurIPS 2022. </li>
        <li> Seunghyuk Cho, Juyong Lee, Dongwoo Kim, <a href="https://arxiv.org/abs/2209.15217">Hyperbolic VAE via Latent Gaussian Distributions</a> NeurIPS 2023. </li>
    </ol>
</div>      
</div>
</li>
	-->

<!--
<li>
  <div class="row" style="padding-top: 40px;">
    <div class="col-sm-2" >
        <img class="img-responsive img-header" src="img/reseach_participate/universe.png" alt="" />
    </div>    
    <div class="col-sm-10">
      <h4 class="header"># Machine Learning for Dynamics System Discovery - Advisor: Dongwoo Kim </h4>
      <p class="normal-text">
        Research to discover underlying dynamical systems using machine learning gets increasing attention to understand and predict various physical phenomena. The main problems are incorporating physical knowledge well into the neural net and handling the various characteristics of real-world data. We are particularly interested in finding the coordinates that provide the most simple representation of the system and dealing with noisy data or data from various environments.
    </p>
    <span class="sub-text"> References </span>
    <ol class="sub-text">
        <li> Yuan Yin, Vincent LE GUEN, Jérémie DONA, Emmanuel de Bezenac, Ibrahim Ayed, Nicolas THOME, patrick gallinari, <a href="https://openreview.net/forum?id=kmG8vRXTFv">Augmenting Physical Models with Deep Networks for Complex Dynamics Forecasting</a> </li>          
        <li> Matthieu Kirchmeyer, Yuan Yin, Jeremie Dona, Nicolas Baskiotis, Alain Rakotomamonjy, Patrick Gallinari, <a href="https://proceedings.mlr.press/v162/kirchmeyer22a.html">Generalizing to New Physical Systems via Context-Informed Dynamics Model</a> </li>          
        <li> Steven L. Brunton, Joshua L. Proctor, J. Nathan Kutz, <a href="https://www.pnas.org/doi/10.1073/pnas.1517384113">Discovering governing equations from data by sparse identification of nonlinear dynamical systems</a> </li>      
    </ol>    
</div>      
</div>
</li>
-->

<li>
  <div class="row" style="padding-top: 40px;">
    <div class="col-sm-2" >
        <img class="img-responsive img-header" src="img/reseach_participate/gdl.png" alt="" />
    </div>    
    <div class="col-sm-10">
      <h4 class="header"># Geometric Deep Learning - Advisor: Sungsoo Ahn </h4>
      <p class="normal-text">
        Geometric deep learning is a new field of machine learning that can learn from complex data like point clouds, graphs, mesh, and manifolds. It seeks for a ``general recipe'' to apply deep neural networks to 3D objects, graphs and manifolds. In this project, we will aim at studying the recent geometric deep learning literature and become the frontiers in this area. We are looking for students confident in mathematics, since since this project requires to study very basic topology, categorical theory, and group theory. See this <a href="https://www.youtube.com/playlist?list=PLn2-dEmQeTfRQXLKf9Fmlk3HmReGg3YZZ">link</a> for a tutorial on this subject.
    </p>
    <span class="sub-text"> References </span>
    <ol class="sub-text">
        <li> Victor Garcia Satorras, Emiel Hoogeboom, Max Welling <a href="https://arxiv.org/abs/2102.09844">E(n) Graph Neural Networks
        </a> </li>
        <li> Jakob Hansen, Thomas Gebhart <a href="https://arxiv.org/abs/2012.06333">Sheaf Neural Networks
        </a> </li>
        <li> Cristian Bodnar, Fabrizio Frasca, Yu Guang Wang, Nina Otter, Guido Montúfar, Pietro Liò, Michael Bronstein <a href="https://arxiv.org/abs/2103.03212">Weisfeiler and Lehman Go Topological: Message Passing Simplicial Networks
        </a> </li>     
    </ol>     
</div>      
</div>
</li>

<li>
  <div class="row" style="padding-top: 40px;">
    <div class="col-sm-2" >
      <img class="img-responsive img-header" src="img/reseach_participate/hallucination.png" alt="" />
  </div>    
  <div class="col-sm-10">
      <h4 class="header"># Learning to Combat the Hallucination of Large Language Models - Advisor: Sangdon Park </h4>
      <p class="normal-text">
        Large Language Models (LLMs) confidently generate wrong information, which undermines the trust of LLMs as a knowledge base. How to mitigate this? 
        This question has been actively emerged due to the power of ChatGPT.
        In this research project, we will explore learning methods (e.g., conformal prediction) to answer this question.
    </p>
    <span class="sub-text"> References </span>
    <ol class="sub-text">
        <li> Andrew Ng’s Twitter Post on <a href="https://twitter.com/AndrewYNg/status/1602725934565830657"> the overconfidence of LLMs</a> </li>
        <li> Sangdon Park and Taesoo Kim <a href="https://arxiv.org/abs/2307.09254">PAC Neural Prediction Set Learning to Quantify the Uncertainty of Generative Language Models</a> arXiv 2023 </li>  
    </ol>
</div>
</div>
</li>

<li>
  <div class="row" style="padding-top: 40px;">
    <div class="col-sm-2" >
      <img class="img-responsive img-header" src="img/reseach_participate/meta.png" alt="" />
  </div>    
  <div class="col-sm-10">
      <h4 class="header"># Uncertainty Learning via Conformal Prediction - Advisor: Sangdon Park </h4>
      <p class="normal-text">
       As ML models are used in practical environments, e.g., ChatGPT or drones, the concerns on the trustworthiness of model predictions have been greatly emerged. In particular, we are interested in rigorous uncertainty learning as the basis for the correctness of predictions, and conformal prediction is a promising method for rigorous uncertainty learning. However, its correctness guarantees depend on the assumptions of data distributions. In this research project, we explore and design practical learning algorithms for conformal prediction under various distributional assumptions.
   </p>
   <span class="sub-text"> References </span>
   <ol class="sub-text">
    <li> Vladimir Vovk et al. <a href="http://alrw.net/">Algorithmic Learning in a Random World</a> 2005 </li>
    <li> Anastasios Angelopoulos et al. <a href="https://arxiv.org/abs/2009.14193">Uncertainty Sets for Image Classifiers using Conformal Prediction</a> ICLR 2021 </li>
    <li> Sangdon Park et al. <a href="https://arxiv.org/abs/2001.00106">PAC Confidence Sets for Deep Neural Networks via Calibrated Prediction</a> ICLR 2020 </li>  
</ol>
</div>
</div>
</li>

<li>
  <div class="row" style="padding-top: 40px;">
    <div class="col-sm-2" >
      <img class="img-responsive img-header" src="img/reseach_participate/llm-security.png" alt="" />
  </div>    
  <div class="col-sm-10">
      <h4 class="header"># Security and Privacy of LLMs - Advisor: Sangdon Park </h4>
      <p class="normal-text">
       The power of LLMs and their daily life use bring concerns on security and privacy issues (e.g., vulnerable code generation and privacy leakage). Then, how severe are the security and privacy issues? How to efficiently and effectively unlearn the issues in LLMs? In this project, we will evaluate LLMs security and privacy concerns and design learning algorithms for mitigation.
   </p>
<!--      <span class="sub-text"> References </span>
      <ol class="sub-text">
        <li> Vladimir Vovk et al. <a href="http://alrw.net/">Algorithmic Learning in a Random World</a> 2005 </li>
	<li> Anastasios Angelopoulos et al. <a href="https://arxiv.org/abs/2009.14193">Uncertainty Sets for Image Classifiers using Conformal Prediction</a> ICLR 2021 </li>
        <li> Sangdon Park et al. <a href="https://arxiv.org/abs/2001.00106">PAC Confidence Sets for Deep Neural Networks via Calibrated Prediction</a> ICLR 2020 </li>  
      </ol>
  -->
</div>
</div>
</li>

<li>
  <div class="row" style="padding-top: 40px;">
    <div class="col-sm-2" >
      <img class="img-responsive img-header" src="img/reseach_participate/security-llm.png" alt="" />
  </div>    
  <div class="col-sm-10">
      <h4 class="header"># LLMs for Code Vulnerability Discovery - Advisor: Sangdon Park </h4>
      <p class="normal-text">
       Finding source or binary code vulnerabilities is a long-standing and never-ending problem. The recent advance in LLMs potentially provides clues to further advance the current code vulnerability discovery performance. In this project, we will explore the potentials of LLMs in finding code vulnerabilities.
   </p>
<!--      <span class="sub-text"> References </span>
      <ol class="sub-text">
        <li> Vladimir Vovk et al. <a href="http://alrw.net/">Algorithmic Learning in a Random World</a> 2005 </li>
	<li> Anastasios Angelopoulos et al. <a href="https://arxiv.org/abs/2009.14193">Uncertainty Sets for Image Classifiers using Conformal Prediction</a> ICLR 2021 </li>
        <li> Sangdon Park et al. <a href="https://arxiv.org/abs/2001.00106">PAC Confidence Sets for Deep Neural Networks via Calibrated Prediction</a> ICLR 2020 </li>  
      </ol>
  -->
</div>
</div>
</li>
</ul>


</div>

