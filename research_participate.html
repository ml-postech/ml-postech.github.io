<script> 
    /* Function to use lastModified property */ 
    document.getElementById("last-updated").innerHTML = formatDate(document.lastModified);

    function formatDate(date) {
    var d = new Date(date),
        month = '' + (d.getMonth() + 1),
        day = '' + d.getDate(),
        year = d.getFullYear();

    if (month.length < 2) 
        month = '0' + month;
    if (day.length < 2) 
        day = '0' + day;

    return [year, month, day].join('-');
    }
</script> 


<div class="content container">
  <div class="row" style="padding-top: 40px;">
    <div class="col-sm-12">
        <h3 class="header"># Research Participation (for Undergrad Students) </h3>
        <p class="sub-text">
        MLLab always welcomes undergraduate students to participate in research. The projects below are the research topics that the lab is currently interested in and intends to proceed. Interested undergraduate students should check the list before applying for research participation and contact their advisor via email. This page will be continuously updated. (Last modified <span id="last-updated"></span>)
        </p>
    </div>
  </div>


  <div class="row" style="padding-top: 40px;">
    <div class="col-sm-2" >
        <img class="img-responsive img-header" src="img/reseach_participate/smiley.png" alt=""  />
    </div>
    <div class="col-sm-10">
        <h4 class="header"># Machine Learning Free Topics - Advisors: Jungseul Ok/Dongwoo Kim/Sungsoo Ahn/Namhoon Lee </h4>
        <p class="normal-text">
            Any other interesting research ideas other than the topics listed below? It looks interesting, but are you wondering if this is also machine learning research? If you want to learn machine learning but don't know where to start, feel free to contact us anytime!
        </p>
    </div>
  </div>

  <div class="row" style="padding-top: 40px;">
    <div class="col-sm-2" >
        <img class="img-responsive img-header" src="img/reseach_participate/unlearning.png" alt=""  />
    </div>
    <div class="col-sm-10">
        <h4 class="header"># Machine Unlearning - Advisors: Jungseul Ok/Dongwoo Kim </h4>
        <p class="normal-text">
            As shown in the recent case of 이루다, machine learning algorithms always have problems that can expose personal and sensitive information. The simplest way to solve this problem is to train the model anew after removing such sensitive information, but this re-learning is only a workaround for solving problems that may arise at any time. Machine Unlearning is a methodology for selectively removing specific information from an already trained model. Students will study what conditions must be satisfied for successful unlearning through participation in this study.
        </p>
        <span class="sub-text"> References <span>
        <ol class="sub-text">
            <li> Nguyen, Quoc Phong, Bryan Kian Hsiang Low, and Patrick Jaillet. <a href="https://arxiv.org/abs/2010.12883">"Variational Bayesian Unlearning."</a> NuerIPS 2020. </li>
            <li> Golatkar, Aditya, Alessandro Achille, and Stefano Soatto. <a href="https://arxiv.org/abs/1911.04933">"Eternal Sunshine of the Spotless Net: Selective Forgetting in Deep Networks."</a> CVPR 2020.</li>
            <li> Yoon, Youngsik, et al. <a href="https://arxiv.org/abs/2205.15567">"Few-Shot Unlearning by Model Inversion."</a> arXiv preprint 2022</li>
        </ol>
    </div>
  </div>

  <div class="row" style="padding-top: 40px;">
    <div class="col-sm-2">
        <img class="img-responsive img-header" src="img/reseach_participate/federated_learning.png" alt=""  />
    </div>
    <div class="col-sm-10">
        <h4 class="header"># Privacy Leakage in Federated Learning - Advisor: Jungseul Ok </h4>
        <p class="normal-text">
            Federated Learning is a machine learning framework that enables multiple users to learn large-scale models without worrying about personal information leakage by enabling learning without sending personal data to a central server. For this large-scale learning, only the learning signal (the differential value of the neural net model) is transmitted to the server, not the data transmission, and according to various recent research results, even this method is not 100% safe from personal information leakage. Participation in this study aims to find out under what circumstances information leakage can be aggravated and what methodologies can be used to solve these problems.
<!--            연합학습은 개인데이터를 중앙 서버로 보내지 않고서도 학습을 가능하게 함으로써 여러 사용자들이 개인정보 유출 걱정없이 대규모 모델을 학습하는데 힘을 보탤수 있는 기계학습 프레임워크 입니다. 이러한 대규모 학습을 위해 데이터 전송이 아닌 학습 시그널(뉴럴넷 모델의 미분값)만을 서버로 전송하는데 최근 다양한 연구결과들에 따르면 이러한 방식조차 개인정보유출로부터 100% 안전한 상황이 아니라는 것이 밝혀졌습니다. 본 연구참여는 어떤 상황에서 정보 유출이 심화될 수 있으며 이러한 문제를 해결하기 위해 어떤 방법론들이 사용될 수 있는지에 대하여 알아보는 것을 목표로 합니다. -->
        </p>
        <span class="sub-text"> References <span>
        <ol class="sub-text">
            <li> Jinwoo Jeon*, Jaechang Kim*, Kangwook Lee, Sewoong Oh, and Jungseul Ok, <a href="https://arxiv.org/abs/2110.14962">"Gradient Inversion with Generative Image Prior"</a>, NeurIPS, 2021  </li>
        </ol>        
    </div>
  </div>


  <div class="row" style="padding-top: 40px;">
    <div class="col-sm-2">
        <img class="img-responsive img-header" src="img/reseach_participate/synthesis.png" alt=""  />
    </div>
    <div class="col-sm-10">
        <h4 class="header"># Generative model for Drug Synthesis - Advisors: Dongwoo Kim/Sungsoo Ahn </h4>
        <p class="normal-text">
            Advances in generative models have been applied to a variety of research fields, including new image creation/composition and text generation. Recently, various studies on the creation/synthesis of new drugs to treat specific diseases have been conducted using these generative models. Through participation in this study, students aim to directly implement/compare various new drug generation models that have been recently proposed, project them, and register them on Github.
        </p>
        <span class="sub-text"> References <span>
        <ol class="sub-text">
            <li> Jin, Wengong, Regina Barzilay, and Tommi Jaakkola. <a href="https://proceedings.mlr.press/v80/jin18a.html">"Junction Tree Variational Autoencoder for Molecular Graph Generation."</a> ICML 2018.</li>
            <li> You, Jiaxuan, et al. <a href="https://proceedings.neurips.cc/paper/2018/hash/d60678e8f2ba9c540798ebbde31177e8-Abstract.html">"Graph Convolutional Policy Network for Goal-Directed Molecular Graph Generation."</a> NeurIPS 2018.</li>
            <li> Sungsoo Ahn, Binghong Chen, Tianzhe Wang, and Le Song, <a href="https://openreview.net/forum?id=w60btE_8T2m">"Spanning Tree-based Graph Generation for Molecules"</a> ICLR 2022. </li>
            <li> Jo, Jaehyeong, Seul Lee, and Sung Ju Hwang. <a href="https://arxiv.org/abs/2202.02514">"Score-based Generative Modeling of Graphs via the System of Stochastic Differential Equations."</a> ICML 2022. </li>
        </ol>
    </div>    
  </div>

   <!--
  <div class="row" style="padding-top: 40px;">
    <div class="col-sm-2">
        <img class="img-responsive img-header" src="img/reseach_participate/skeleton.png" alt=""  />
    </div>
    <div class="col-sm-10">
        <h4 class="header"># 스켈레톤 분석을 통한 나이/성별 예측 - 지도교수: 김동우 </h4>
        <p class="normal-text">
        	최근 그래프 뉴럴넷은 스켈레톤 기반의 행동 분류(action recognition)를 위해 다양하게 사용되고 있습니다. 본 연구에서는 이러한 스켈레톤 기반의 행동 분류를 확장하여 판별 대상의 성별/나이 등을 인식할 수 있을지 예측하는 모델을 설계하는 것을 목표로 합니다. 본 연구참여는 이러한 연구들을 통해 최종적으로 대상의 질병을 진단하고 판별하는 것이 가능할 것인가에 대한 예비 연구의 형태를 띄고 있습니다. 추가적으로 개인정보 보호를 위해 나이/성별 등의 민감한 정보가 예측이 되면 안되는 상황에 대해 연구하고 이를 막기 위해 어떠한 알고리즘을 구성해야할지에 대해 고민해 보는것을 목표로 합니다.
        </p>
    </div>    
  </div>

  <div class="row" style="padding-top: 40px;">
    <div class="col-sm-2">
        <img class="img-responsive img-header" src="img/reseach_participate/network.png" alt=""  />
    </div>
    <div class="col-sm-10">
        <h4 class="header"># 그래프 수정을 통한 그래프 뉴럴넷 성능 향상 - 지도교수: 김동우 </h4>
        <p class="normal-text">
        	그래프 뉴럴넷은 그래프를 입력으로 받아서 다양한 예측 태스크를 수행하는 뉴럴넷의 한 종류로 노드 분류 / 연결성 예측 / 그래프 분류 등을 위해 다양하게 쓰이고 있습니다. 현재 제안되고 있는 대다수의 그래프 뉴럴넷은 입력으로 주어지는 그래프 자체가 가지는 오류(noise)에 대해서는 크게 주의를 기울이고 있지 않습니다. 하지만 입력 그래프 자체가 많은 오류를 가지고 있다면 최종 학습 성능은 더 떨어지게 될 것입니다. 본 연구는 이런 문제를 해결하기 위해 어떤 방식으로 입력 그래프를 수정해야 그래프가 가지고 있는 오류를 줄여 최종 예측 성능을 더 높일 수 있을지에 대해 연구하는 것이 목표로 참여 학생들은 이를 통해 그래프 뉴럴넷, 최적화 등에 대해 더 깊게 이해할 수 있을 것입니다.
        </p>
    </div>    
  </div>  
-->
    
  <div class="row" style="padding-top: 40px;">
    <div class="col-sm-2">
        <img class="img-responsive img-header" src="img/reseach_participate/implicit_function.png" alt=""  />
    </div>
    <div class="col-sm-10">
        <h4 class="header"># Implicit Function for Natural Data (Sound/Image) - Advisor: Jungseul Ok </h4>
        <p class="normal-text">
            Traditionally, natural signals have been presented discretely, such as pixels in images and sampling frequencies in sound. However, if the implicit function is used, the analog signal of nature can be expressed continuously. Specifically, it represents the coordinate values and their corresponding signals as a continuous function. In this way, natural data such as sound/image can be expressed regardless of resolution, and furthermore, infinite resolution expression can be expected. This study aims to find out what methodologies can be used to effectively map natural data to implicit functions and aims to utilize INR for various tasks by increasing its representation power.
<!--        	전통적으로 자연 신호는 이미지에서의 픽셀, 소리에서의 샘플링 주파수와 같이 불연속적으로 표현되었습니다. 그러나 implicit function을 활용한다면 자연의 아날로그 신호를 연속적으로 표현할 수 있습니다. 구체적으로 좌표 값과 이에 대응하는 신호를 연속 함수로 나타냅니다. 이와 같은 방식을 통해 소리/이미지와 같은 자연 데이터를 해상도(resolution)에 구애 받지 않고 나타낼 수 있고, 더 나아가서 무한의 해상도(resolution) 표현도 기대할 수 있습니다. 본 연구는 자연 데이터를 implicit function으로 효과적으로 맵핑하기 위해서 어떠한 방법론들이 사용될 수 있는 지에 대하여 알아보는 것을 목표로 합니다. -->
        </p>
        <span class="sub-text"> References <span>
        <ol class="sub-text">
            <li> Vincent Sitzmann, et al. <a href="https://arxiv.org/abs/2006.09661">"Implicit Neural Representations with Periodic Activation Functions"</a> NeurIPS 2020. </li>
            <li> Ben Mildenhall, et al. <a href="https://arxiv.org/abs/2003.08934">"NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis"</a> ECCV 2020. </li>
            <li> Jaechang Kim*, Yunjoo Lee*, Seunghoon Hong, and Jungseul Ok, <a href="https://arxiv.org/abs/2111.00195">"Learning Continuous Representation of Audio for Arbitrary Scale Super Resolution" </a>, ICASSP 2022.  </li>
        </ol>        
    </div>    
  </div> 

   <div class="row" style="padding-top: 40px;">
    <div class="col-sm-2">
        <img class="img-responsive img-header" src="img/reseach_participate/DRL.png" alt=""  />
    </div>
    <div class="col-sm-10">
        <h4 class="header"># Deep Reinforcement Learning with Prior- Advisor: Jungseul Ok </h4>
        <p class="normal-text">
            Deep reinforcement learning (DRL) is a learning method which approximates functions in traditional reinforcement learning (RL) algorithms using deep neural networks. By virtue of deep learning, DRL can outperform traditional RL methods and solve problems difficult for traditional RL to deal with. However, low data efficiency and poor generalization performance remain major challenges in DRL. In this work, students aim to increase sample-efficiency of DRL using pre-existing algorithms, or to increase generalization performance using prior knowledge like data augmentation or Lipschitz continuity.
        </p>
        <span class="sub-text"> References <span>
        <ol class="sub-text">
            <li> Krishan Rana, et al. <a href="https://arxiv.org/abs/2003.05117">"Multiplicative Controller Fusion: Leveraging Algorithmic Priors for Sample-efficient Reinforcement Learning and Safe Sim-To-Real Transfer"</a> IROS 2020. </li>
            <li> Nicklas Hansen, and Xiaolong Wang. <a href="https://arxiv.org/abs/2011.13389">"Generalization in Reinforcement Learning by Soft Data Augmentation"</a> ICRA 2021. </li>
            <li> Roberta Raileanu, and Rob Fergus.<a href="https://arxiv.org/abs/2102.10330">"Decoupling Value and Policy for Generalization in Reinforcement Learning"</a> ICML 2021. </li>
        </ol>
    </div>    
  </div> 

  <div class="row" style="padding-top: 40px;">
    <div class="col-sm-2" >
        <img class="img-responsive img-header" src="img/reseach_participate/crowdsourcing.png" alt="" />
    </div>
    <div class="col-sm-10">
        <h4 class="header"># Deep Learning from Crowds - Advisor: Jungseul Ok </h4>
        <p class="normal-text">
            Crowdsourcing enables us to collect large-scale datasets for machine learning at a low cost. However, low-paid workers easily provide noisy labels due to mistakes or malice, which undermines the performance of neural networks. Furthermore, workers can provide only a few noisy labels, and they want to receive tasks that are easy to label. In this work, we aim to design frameworks that take these realistic behaviors of workers into account.
        </p>
        <span class="sub-text"> References <span>
        <ol class="sub-text">
            <li> Cai, Lile, et al. <a href="https://openaccess.thecvf.com/content/CVPR2021/html/Cai_Revisiting_Superpixels_for_Active_Learning_in_Semantic_Segmentation_With_Realistic_CVPR_2021_paper.html">"Revisiting Superpixels for Active Learning in Semantic Segmentation with Realistic Annotation Costs."</a> CVPR 2021. </li>
            <li> Hoyoung Kim*, Seunghyuk Cho*, Dongwoo Kim, and Jungseul Ok, <a href="https://arxiv.org/abs/2111.00734">"Robust Deep Learning from Crowds with Belief Propagation."</a> AISTATS 2022. </li>            
        </ol>
    </div>
  </div>

  <div class="row" style="padding-top: 40px;">
    <div class="col-sm-2" >
        <img class="img-responsive img-header" src="img/reseach_participate/gnn.png" alt="" />
    </div>
    <div class="col-sm-10">
        <h4 class="header"># Graph Neural Network - Advisors: Sungsoo Ahn / Dongwoo Kim </h4>
        <p class="normal-text">
            Graph neural networks (GNNs) generalize the existing deep learning techniques to graph structured data including biological networks, molecular graphs, academic networks, and knowledge graphs. In this project, we aim to improve the existing GNNs to be more expressive and sample-efficient. We are particularly interested in directions like architecture design, data augmentation, and self-supervised learning. 
        </p>
        <span class="sub-text"> References <span>
        <ol class="sub-text">
            <li> Kipf, Thomas N., and Max Welling. <a href="https://arxiv.org/abs/1609.02907">"Semi-supervised Classification with Graph Convolutional Networks."</a> ICLR 2017. </li>
            <li> Xu, Keyulu, et al. <a href="https://openreview.net/forum?id=ryGs6iA5Km">"How Powerful are Graph Neural Networks?."</a> ICLR 2019. </li>            
        </ol>
    </div>
  </div>

  <div class="row" style="padding-top: 40px;">
    <div class="col-sm-2" >
        <img class="img-responsive img-header" src="img/reseach_participate/design.png" alt="" />
    </div>
    <div class="col-sm-10">
        <h4 class="header"># Machine Learning for Design - Advisor: Sungsoo Ahn </h4>
        <p class="normal-text">
            Many important problems in real-world includes inventing a new concept or an object. Examples include drug discovery, material design, architecture search for deep neural network, and robot morphology optimization. We are interested in developing new algorithms for this problem based on reinforcement learning, conditional generation, and model-based optimization. 
        </p>
        <span class="sub-text"> References <span>
        <ol class="sub-text">
            <li> Sungsoo Ahn, Junsu Kim, Hankook Lee, Jinwoo Shin, <a href="https://proceedings.neurips.cc//paper/2020/hash/8ba6c657b03fc7c8dd4dff8e45defcd2-Abstract.html">"Guiding Deep Molecular Optimization with Genetic Exploration"</a> NeurIPS 2020. </li>
            <li> Sihyun Yu, Sungsoo Ahn, Le Song, Jinwoo Shin, <a href="https://proceedings.neurips.cc/paper/2021/hash/24b43fb034a10d78bec71274033b4096-Abstract.html">"RoMA: Robust Model Adaptation for Offline Model-based Optimization"</a> NeurIPS 2021. </li>
            <li> Dinghuai Zhang, Jie Fu, Yoshua Bengio, Aaron Courville, <a href="https://openreview.net/forum?id=1HxTO6CTkz">"Unifying Likelihood-free Inference with Black-box Optimization and Beyond."</a> ICLR 2022. </li>
        </ol>        
    </div>
  </div>

  <div class="row" style="padding-top: 40px;">
    <div class="col-sm-12">
        <h4 class="header"># Deep Learning with Algorithmic Prior - Advisor: Sungsoo Ahn </h4>
        <p class="normal-text">
            Algorithms designed by scientists and mathematicians provide strong domain knowledge for solving many problems. We are interested in improving deep learning-based algorithms using the intuitions from algorithms like genetic algorithm, A*-search, Monte Carlo tree search, and survey propagation. 
        </p>
        <span class="sub-text"> References <span>
        <ol class="sub-text">
            <li> Sungsoo Ahn, Younggyo Seo, Jinwoo Shin, <a href="https://proceedings.mlr.press/v119/ahn20a.html">"Learning What to Defer for Maximum Independent Sets"</a> ICML 2020. </li>
            <li> Sungsoo Ahn, Junsu Kim, Hankook Lee, Jinwoo Shin, <a href="https://proceedings.neurips.cc//paper/2020/hash/8ba6c657b03fc7c8dd4dff8e45defcd2-Abstract.html">"Guiding Deep Molecular Optimization with Genetic Exploration"</a> NeurIPS 2020. </li>
            <li> Junsu Kim, Sungsoo Ahn, Hankook Lee, Jinwoo Shin, <a href="https://proceedings.mlr.press/v139/kim21b">"Self-Improved Retrosynthetic Planning"</a> ICML 2021. </li>
        </ol>        
    </div>
  </div>
     
  <div class="row" style="padding-top: 40px;">
      <div class="col-sm-12">
          <h4 class="header"># JAX - Advisor: Namhoon Lee </h4>
          <p class="normal-text">
          <a href="https://github.com/google/jax">JAX</a> is a new framework developed by Google for high-performance ML research;
          it is becoming increasingly more popular with its acceleration and being adopted by major subfields that require large-scale scientific computing.
          Student researchers on this project will work on implementing and evaluating ML algorithms using JAX, and the topics will include, but not be limited to, deep learning and distributed optimization.
          This post is ideally for CS majors or those who have some experience in functional programming.
          </p>
      </div>
  </div>

             
  <div class="row" style="padding-top: 40px;">
      <div class="col-sm-12">
          <h4 class="header"># Machine Learning in Non-Euclidean Spaces - Advisor: Dongwoo Kim </h4>
          <p class="normal-text">
            Most of well-known machine learning algorithms are working in Euclidean space. However, many scientific fields study data whose underlying structure is non-Euclidean. Through this research paticipation, students will study different types of non-Euclidean spaces and develop a new machine learning algorithm working in these spaces.
          </p>
        <span class="sub-text"> References <span>
        <ol class="sub-text">
            <li> Seunghyuk Cho, Juyong Lee, Jaesik Park, Dongwoo Kim, <a href="https://arxiv.org/abs/2205.13371">A Rotated Hyperbolic Wrapped Normal Distribution for Hierarchical Representation Learning</a> </li>          
      </div>      
  </div>

  <div class="row" style="padding-top: 40px;">
  </div>
    
</div>

