---
title: Foundational Datasets for VLMs 
advisor: advisor-jungseul-ok
mentor: student-hoyoung-kim
---
Vision-Language Models (VLMs) leverage large-scale multimodal datasets, such as image-text and video-text pairs, to learn cross-modal representations. While the abundance of data has driven significant advancements, it also presents challenges. First, excessive data volume increases computational costs and reduces training efficiency, making it difficult to extract high-quality representations. Additionally, such datasets contain noisy annotations, redundant samples, and domain biases, all of which can negatively impact model performance. Our research aims to evaluate the effect of dataset quality on VLM training, develop filtering techniques to eliminate noisy and redundant data, and explore strategies for generating high-quality synthetic data to enhance learning efficiency.
