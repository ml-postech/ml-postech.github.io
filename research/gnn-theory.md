---
title: Theoretical Insights into How Neural Network Architectures Affect Performance
advisor: advisor-dongwoo-kim
mentors:
- student-moonjeong-park
---
Neural network architectures implicitly encode priors that influence how models understand data and generalize to new examples. Convolutional networks, for example, embed translational invariance suitable for visual perception, while graph neural networks assume relational smoothness between connected nodes. These structural biases improve performance in a parameter-efficient manner but can also introduce side effects, such as the oversmoothing phenomenon in GNNs. Therefore, a precise understanding of how architectural design influences learning is essential for developing robust and effective neural networks.
This research aims to uncover the implicit biases underlying various neural network architectures and to analyze their effects from multiple theoretical perspectives, including learning dynamics, optimization stability, and generalization error.
Such understanding reveals when and why models perform well or fail, guiding the design of more robust architectures.
